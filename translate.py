#!/usr/bin/env python3
"""
    This module is designed to ease the migration and entry of translations into WD tenants.

    It is designed to handle two types of files:
        - populated xml templates generated by WD
        - csv files (usually from excel) generated by the user

    It can take one or more sources and combine them all into one final output file.

    The output file can be both / either wd xml format or csv format

    You can specify the priority of the sources (e.g. if two files have a translation for the same string, which one wins)

    You can do a diff to see what strings are different from one file to the other

    This program can handle those WD objects that are "WID". It does this by matching the class name, name and the string to
    be translated. If the ref id is not WID, then it uses the class name, name and ref id to match across environments.

    The user generated csv file must have EXACT matches for these items. Either for the ref ids or for the strings if WID.

    TODO:
        Clean up old code, remove unused imports, comment, add help
        Add database support
        Implement REST integration 
        Lots more comments
        Logging - validation, error, info
        Add timing count down

"""
from .__init__ import *
import sys
import argparse
import os.path
import codecs
import csv as csv_module
from .wd_languages import Translatable_Class_Attribute_DataType, \
    User_LanguageObjectIDType, User_LanguageObjectType, Attribute_ReferenceType, \
    Translated_Value_for_Instance_DataType, Instance_IDType, InstanceObjectType, \
    Put_Translatable_Tenant_Data_RequestType, rootType, parse
from lxml import etree
from .translation import Translatable_Item, Translatable_Source, Source_Language_Translated_Value, Translatable_Class

def parse_command_line(cmd_args):
    parser = argparse.ArgumentParser(description=("Takes translation files from one tenant and"
            " puts the values into the file from another tenant"))

    parser.add_argument("-xml-source", nargs="+", metavar="<Source file in wd xml format>", help=("This is the source file"
            "- the file that has the translated values."), default=[])
    parser.add_argument("-csv-source", nargs="+", metavar="<Source file in csv format>", help=("This is the source file"
            "- the file that has the translated values."), default=[])
    parser.add_argument("-xml-dest", metavar="<Destination file in wd xml format>", help=("This is the destination file"
            "- the file that has the translated values."))
    parser.add_argument("-respect", default=False, action="store_true", help=("Respects translated "
            "values in the destination tenant. Will not overwrite them"))
    parser.add_argument("-pretty", default=False, action="store_true", help=("Generates copies of "
            "source and destination xml files in human readable format (with spacing). *DO NOT* use "
            "these files as a source file for Workday as the spacing will break things. File will be "
            "the original file name with PRETTY as suffix before the .xml."))
    parser.add_argument("-v", "--validate_self", default=False, action="store_true", help=(
            "Perform validations against files. Current validations are to check for "
            "inconsistent translations and to check for source reference ids that do not "
            "exist in destination."))
    parser.add_argument("-all_lines", default=False, action="store_true", help=("Default behavior "
            "is to remove all lines from destination file that do not contain translatable values, "
            "use this flag if you want all lines included in the destination file."))

    return parser.parse_args(cmd_args)

"""
    Convenience functions
"""
def p(e): return u"{}".format(etree.tostring(e, pretty_print=True))
def pr(e): return u"{}".format(etree.tostring(e, pretty_print=False))
def status(msg):
    global start_time
    global last_update_time

    now = int(time.time())
    time_elapsed = now - start_time
    time_since_last_update = now - last_update_time
    info(u"{} ({})".format(msg, time_elapsed))
    last_update_time = now
    return

def csv(args):
    """
        Open the csv file and read into shortcut object
        Open xml and and build tenant object
            Iterate through translatable tenant data until we find matching class
            copy one for each target language and put in the translations
            Row looks like this;
            class name, name, reference type, reference ID, rich text (y/n), for translation, lang id, ...
    """
    translatable_item_set = set()

    encodings_to_try = ["utf_16", "utf_8"]
    success_with_encoding = False
    for encoding in encodings_to_try:
        try:
            with open(args.source_file, "rU", encoding=encoding) as csvfile:
                source = Translatable_Source(args.source_file)
                reader = csv_module.reader(csvfile)
                # First row is a head row that will give us the language tags
                CLASS_NAME_I = 0
                NAME_I = 1
                REFERENCE_TYPE_I = 2
                REFERENCE_ID_I = 3
                RICH_TEXT_FLAG_I = 4
                BASE_VALUE_I = 5
                TRANSLATION_START_I = 6
                lang_list = next(reader)[TRANSLATION_START_I:]
                info("The language list is: {}".format(lang_list))
                for row in reader:
                    if not row[CLASS_NAME_I]:
                        continue  # Skip blank lines, class name required
                    if row[RICH_TEXT_FLAG_I] in ["Y", "y", "T"]:
                        rich_flag = True
                        rich_base_value = row[BASE_VALUE_I]
                        base_value = None
                    else:
                        rich_flag = False
                        base_value = row[BASE_VALUE_I]
                        rich_base_value = None
                    try:
                        t = Translatable_Item(row[CLASS_NAME_I], row[NAME_I], row[REFERENCE_TYPE_I],
                                row[REFERENCE_ID_I], None, None, base_value, rich_base_value, source)
                    except:
                        print(row)
                        raise
                    translatable_item_set.add(t)

                    # Now go through translations by language
                    for index, lang in enumerate(lang_list, TRANSLATION_START_I):
                        val = row[index]
                        if val:  # If value is not empty, then
                            if rich_flag:
                                translated_value = None
                                rich_translated_value = val
                            else:
                                translated_value = val
                                rich_translated_value = None
                            t.add_translation(source, lang, translated_value, rich_translated_value)
            # End of with block
            success_with_encoding = True
            break  # Break out of for loop for encodings
        except UnicodeError:
            info("Tried {} encoding and failed".format(encoding))
            continue
    if not success_with_encoding:
        # No good encodings found
        raise UnicodeError("No encodings found that worked with supplied file")

    translatable_item_set |= set(load_workday_xml_file(args.destination_file, only_load_translations_flag=True))

    # Now go through each item, which is guaranteed to have at least one language translated, and generate file
    outfilename = "{}.WITH_TRANSLATIONS{}".format(os.path.splitext(args.destination_file)[0],os.path.splitext(args.destination_file)[1])
    with open(outfilename, "w", encoding="utf_8") as f:
        status("Writing output file: {}".format(outfilename))
        f.write('<?xml version="1.0" encoding="utf-8"?>')
        ## Not really considering source at this point. That is critical
        r = rootType()
        p = Put_Translatable_Tenant_Data_RequestType()
        sources_of_translations_with_no_destination_set = set()
        for (language, class_name, name, source), source_language_translated_value_list in Source_Language_Translated_Value.by_language_class_name_name_and_source.items():
            destination = Translatable_Source.dict[args.destination_file]
            l = User_LanguageObjectIDType("User_Language_ID", language)
            user_language_reference = User_LanguageObjectType()
            user_language_reference.add_ID(l)
            attribute_reference = Attribute_ReferenceType(name, destination.namespace)
            a = Translatable_Class_Attribute_DataType(user_language_reference, class_name, attribute_reference)
            for source_language_translated_value in source_language_translated_value_list:
                if not source_language_translated_value.has_source(destination):
                    sources_of_translations_with_no_destination_set.add((destination, class_name, name))
                    validation("Translation {} does not have a target in destination tenant file".format(source_language_translated_value.parent))
                    continue
                ref_id = source_language_translated_value.ref_id(destination)
                ref_id_type = source_language_translated_value.ref_id_type(destination)
                parent_ref_id = source_language_translated_value.parent_ref_id(destination)
                parent_ref_id_type = source_language_translated_value.parent_ref_id_type(destination)
                base_value = source_language_translated_value.base_value
                rich_base_value = source_language_translated_value.rich_base_value
                translated_value = source_language_translated_value.translated_value
                rich_translated_value = source_language_translated_value.rich_translated_value
                instance_id = Instance_IDType(parent_ref_id, parent_ref_id_type, ref_id_type, ref_id)
                instance_obj = InstanceObjectType()
                instance_obj.add_ID(instance_id)
                translated_value_for_instance_data = Translated_Value_for_Instance_DataType(instance_obj, base_value, translated_value, rich_base_value, rich_translated_value)
                a.add_Translated_Value_for_Instance_Data((translated_value_for_instance_data))
            p.add_Translatable_Tenant_Data_Data(a)
        r.add_Put_Translatable_Tenant_Data_Request((p))
        r.export(f, 100, pretty_print=False)

    # Print a validation report showing the class name / name base values in destination tenant where there were no translations
    for (destination, class_name, name) in sources_of_translations_with_no_destination_set:
        for translatable_item in Translatable_Item.by_source_class_name_and_name[(destination, class_name, name)]:
            if not translatable_item.has_translation:
                validation(("Translatable item |{}| in destination file does not have a matching translation in the source file AND source file"
                           " has translations with the same class and class name with no match").format(translatable_item))

    pretty_fname = "{}.PRETTY{}".format(os.path.splitext(args.destination_file)[0],os.path.splitext(args.destination_file)[1])
    with open(pretty_fname, "w", encoding="utf_16") as f:
        status("Writing pretty output file: {}".format(pretty_fname))
        #f.writelines(etree.tostring(dest_tenant.tree.getroot(), encoding="unicode", pretty_print=True))

    return

def iload(args):
    """
    The args should be a list of one or more file names. The files should be WD xml formatted files
    Open the file, create a tenant object, and generate the csv
    :param args:
    :return:
    """
    for fname in args.files_to_convert:
        new_fname = "{}.{}".format(os.path.splitext(fname)[0], "csv")
        tenant = build_tenant(fname, new_fname)
        if not args.all_records:
            tenant.remove_untranslated_instances()
        status("Generating csv file named {}".format(new_fname))
        generate_csv_file(new_fname, tenant)
    return

def generate_xml_file(source):
    """
        This file takes a source (Translation_Source) and generates a Workday xml file. It uses logic / objects
        created by generateDS.py - the same logic used to parse the data from a xml file. The file was modified to
        remove the encapsulating Put request (that's only needed when using REST, not the GUI)
    """
    assert type(source) == Translatable_Source, "Invalid type passed for source"
    with open(source.source_name, "w", encoding="utf_8") as f:
        status("Writing output file: {}".format(source.source_name))
        f.write('<?xml version="1.0" encoding="utf-8"?>')
        ## Not really considering source at this point. That is critical
        r = rootType()
        for klass in source.get_my_classes():
            for lang in klass.get_languages():
                l = User_LanguageObjectIDType("User_Language_ID", lang)
                user_language_reference = User_LanguageObjectType()
                user_language_reference.add_ID(l)
                attribute_reference = Attribute_ReferenceType(klass.name, klass.namespace)
                a = Translatable_Class_Attribute_DataType(user_language_reference, klass.class_name, attribute_reference)
                for ti in source.get_translatable_items_by_class(klass):  # ti = Translatable Item
                    for sltv in ti.get_source_language_translated_values(source, lang):  # sltv = Source Language Translated Value
                        (ref_id_type, ref_id, parent_ref_id_type, parent_ref_id) = ti.get_source_ids(source)
                        base_value = ti.base_value
                        rich_base_value = ti.rich_base_value
                        translated_value = sltv.translated_value
                        rich_translated_value = sltv.rich_translated_value
                        instance_id = Instance_IDType(parent_ref_id, parent_ref_id_type, ref_id_type, ref_id)
                        instance_obj = InstanceObjectType()
                        instance_obj.add_ID(instance_id)
                        translated_value_for_instance_data = Translated_Value_for_Instance_DataType(instance_obj, base_value, translated_value, rich_base_value, rich_translated_value)
                        a.add_Translated_Value_for_Instance_Data((translated_value_for_instance_data))
            r.add_Translatable_Tenant_Data_Data(a)
        r.export(f, 100, pretty_print=False)
    return

def generate_csv_file(file_name, tenant):
    """
        Given a tenant, generate a csv file with named file_name
        csv file is designed to be copied directly into an iLoad file
    :param file_name:
    :param tenant:
    :return:
    """
    with codecs.open(file_name, "w", encoding="utf-8") as f:
        row_ctr = 0
        for row in tenant.get_csv_string():
            f.write(row)
            row_ctr += 1
            if not row_ctr % 25:
                break
                status("Class number {}".format(row_ctr))
            del(row)
    sys.exit()
    return

def parse_csv_file(fname, ignore_untranslated_items=False):
    """
        Open the supplied csv file and return a list of translation items
    """
    ret_list = []
    encodings_to_try = ["utf_16", "utf_8"]
    success_with_encoding = False
    for encoding in encodings_to_try:
        try:
            with open(fname, "rU", encoding=encoding) as csvfile:
                source = Translatable_Source(fname)
                reader = csv_module.reader(csvfile)
                # First row is a head row that will give us the language tags
                CLASS_NAME_I = 0
                NAME_I = 1
                REFERENCE_TYPE_I = 2
                REFERENCE_ID_I = 3
                RICH_TEXT_FLAG_I = 4
                BASE_VALUE_I = 5
                TRANSLATION_START_I = 6
                lang_list = next(reader)[TRANSLATION_START_I:]
                info("From file {}, the language list is: {}".format(fname, lang_list))
                for row in reader:
                    if not row[CLASS_NAME_I]:
                        continue  # Skip blank lines, class name required
                    if row[RICH_TEXT_FLAG_I] in ["Y", "y", "T"]:
                        rich_flag = True
                        rich_base_value = row[BASE_VALUE_I]
                        base_value = None
                    else:
                        rich_flag = False
                        base_value = row[BASE_VALUE_I]
                        rich_base_value = None
                    try:
                        t = Translatable_Item(row[CLASS_NAME_I], row[NAME_I], row[REFERENCE_TYPE_I],
                                              row[REFERENCE_ID_I], None, None, base_value, rich_base_value, source)
                    except:
                        print(row)
                        raise
                    ret_list.append(t)

                    # Now go through translations by language
                    for index, lang in enumerate(lang_list, TRANSLATION_START_I):
                        val = row[index]
                        if val:  # If value is not empty, then
                            if rich_flag:
                                translated_value = None
                                rich_translated_value = val
                            else:
                                translated_value = val
                                rich_translated_value = None
                            t.add_translation(source, lang, translated_value, rich_translated_value)
            # End of with block
            success_with_encoding = True
            break  # Break out of for loop for encodings
        except UnicodeError:
            info("Tried {} encoding and failed".format(encoding))
            continue
    if not success_with_encoding:
        # No good encodings found
        raise UnicodeError("No encodings found that worked with supplied file")

    return ret_list

def parse_xml_file(fname, ignore_untranslated_items=False):
    """
        This parses a Workday xml export file. It uses logic generated by GenerateDS.py, which reads the xsd schema
        and generates matching WD classes. There is one change, the xsd is for REST service, and there is one top level
        object that needs to be bypased for the xml iLoad. So the xsd was manually changed so that the root points to the
        translatable classes, and not the put request. Then generateDS.py was run and the resulting objects are used.
        Returns a list of Translatable_Item(s)
    """
    ret_list = []

    # Create a source
    source = Translatable_Source(fname)

    # This is the function generated by generateDS.py, it parses the file into memory
    root_object = parse(fname, True)

    # Get the list of translated classes
    translated_classes_list = root_object.get_Translatable_Tenant_Data_Data()

    # Now go through the list and build items as needed
    # The code is kind of "heavy" - but I wanted to use the field names for purposes of clarity and maintainability
    ctr = 0
    for klass in translated_classes_list:
        lang = klass.get_User_Language_Reference().get_ID()[0].get_valueOf_()
        klass_class_name = klass.get_Class_Name()
        klass_name = klass.get_Attribute_Reference().get_Name()
        klass_namespace_URI = klass.get_Attribute_Reference().get_Namespace_URI()
        if False:
            debug("Class name |{}| name |{}| namespace |{}| Lang |{}|".format(klass_class_name, klass_name, klass_namespace_URI, lang))
        for translatable_values in klass.get_Translated_Value_for_Instance_Data():
            ctr += 1
            base_value = translatable_values.get_Base_Value()
            rich_base_value = translatable_values.get_Rich_Base_Value()
            translated_base_value = translatable_values.get_Translated_Value()
            translated_rich_base_value = translatable_values.get_Translated_Rich_Value()
            instance_reference = translatable_values.get_Instance_Reference().get_ID()[0]
            #ir = translatable_values.get_Instance_Reference()
            ref_type = instance_reference.get_wd_type()
            ref_value = instance_reference.get_valueOf_()
            parent_ref_type = instance_reference.get_wd_parent_type()
            parent_ref_id = instance_reference.get_wd_parent_id()
            if False:
                debug("base |{}| rich |{}| tbase |{}| trich |{}| ref type |{}| ref val |{}| parent type |{}| parent val |{}| line {}".format(
                        base_value, rich_base_value, translated_base_value, translated_rich_base_value, ref_type, ref_value, parent_ref_type, parent_ref_id, ctr
                ))
            if ignore_untranslated_items:
                if not translated_rich_base_value and not translated_base_value:
                    continue

            ret_list.append(Translatable_Item(klass_class_name, klass_name, ref_type, ref_value, parent_ref_type,
                    parent_ref_id, base_value, rich_base_value, source, lang, translated_base_value,
                    translated_rich_base_value, klass_namespace_URI))

    debug("In parse_xml_file: Processed {}. Number of translatable items generated = {}".format(fname, len(ret_list)))
    return ret_list

def main(cmd_args):
    global args, start_time, last_update_time

    debug("Called with arguments: {}".format(cmd_args))

    # Do some setup
    start_time = int(time.time())
    last_update_time = start_time

    args = parse_command_line(cmd_args)
    debug("args {}".format(args))

    results_dict = dict()

    # Go through all the Workday xml source files, usually one per language
    if args.xml_source:
        for fname in args.xml_source:
            info("Processing {}...".format(fname))
            results_dict[fname] = parse_xml_file(fname, True)

    # Go through any csv files that were provided - these are also source files
    if args.csv_source:
        for fname in args.csv_source:
            info("Processing {}...".format(fname))
            results_dict[fname] = parse_csv_file(fname, True)

    # If there is a target Workday tenant, then we need the WIDs. So load the entire file even those items without
    # translations
    if args.xml_dest:
        missing_destination = set()
        destination = Translatable_Source(args.xml_dest) # This will return the existing destination
        info("Processing {} ...".format(args.xml_dest))
        results_dict[args.xml_dest] = parse_xml_file(args.xml_dest, False)

        # Now we are going to perform some validations
        # Source translations with no destination
        for fname in args.csv_source + args.xml_source:
            for translatable_item in results_dict[fname]:
                if not translatable_item.has_source(destination):
                    missing_destination.add((fname, destination, translatable_item))
        if missing_destination and True:  # Add a command line option  to turn this on/off
            no_dest_err_fname = "No_Matching_Destination_Found.txt"
            info("There are {} translated items that have no destination. See details in {}".format(len(missing_destination), no_dest_err_fname))
            with open(no_dest_err_fname, "a", encoding="utf_8") as f:
                f.write("Processed on {}\n".format(time.asctime()))
                f.write("Data in the format of source of translation source|destination for translation|translation details\n")
                for err in missing_destination:
                    f.write("{}|{}|{}\n".format(err[00], err[1], err[2]))

        # Migrate translations and optimize - we are going to migrate any translations that have a translation AND exist
        # in the destination. We are moving them to a "new destination' using the ref ids from the user supplied destination
        new_source = Translatable_Source("{}.WITH_TRANSLATIONS{}".format(os.path.splitext(args.xml_dest)[0], os.path.splitext(args.xml_dest)[1]))
        new_list = set()
        for translatable_item in results_dict[args.xml_dest]:
            if translatable_item.has_translation:
                new_list.add(translatable_item.migrate_translation(new_source, destination, Translatable_Item.Migration_Strategy.FIRST_AVAILABLE))
        info("New source created of {}".format(new_source))
        info("Translatable item count: {}".format(new_source.item_count))

        # Export the new source
        generate_xml_file(new_source)

    return

if __name__ == "__main__":
    main(sys.argv[1:])


